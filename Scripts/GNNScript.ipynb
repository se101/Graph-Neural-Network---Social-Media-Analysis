{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from torch_geometric.data import HeteroData\n",
    "import regex as re\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.nn import GATConv, Linear, to_hetero\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0940a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/scratch/dipanjan/Twitter_Analysis/Combined_tweet/Combined_Tweets/\",\"/scratch/dipanjan/Twitter_Analysis/Combined_tweet/Combined_Retweets/\"]\n",
    "hashtagcode_path = \"List_of_Daily_Hashtags_Coded.csv\"\n",
    "embedding_name_path = \"Naming_dict_tweets_embeddings.csv\"\n",
    "embedding_name_pathr = \"Naming_dict_retweets_embeddings.csv\"\n",
    "embed_path = '/scratch/dipanjan/Twitter_Analysis/Embeddings/Combined_Tweet_Embeddings/Embeddings/'\n",
    "rembed_path = '/scratch/dipanjan/Twitter_Analysis/Embeddings/Combined_Tweet_Embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25290533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hashtagdict_rdlist():\n",
    "    rt_df = pd.read_csv(hashtagcode_path, dtype=object)\n",
    "    rt_df = rt_df.drop(rt_df.columns[[2, 3]], axis=1)\n",
    "    rt_df = rt_df.dropna(subset=['Final Code'])\n",
    "    readlist = rt_df.Hashtag.values.tolist()\n",
    "    rdlist = []\n",
    "    for it in readlist:\n",
    "        rdlist.append(it)\n",
    "    coding_to_num = {'E': 0, 'G': 1, 'I': 2, 'M': 3, 'S': 4}\n",
    "\n",
    "    coddf = rt_df.replace({\"Final Code\": coding_to_num})\n",
    "    hashtag_dict = coddf.set_index('Hashtag').to_dict()['Final Code']\n",
    "    rdlist = rdlist\n",
    "    ret_hr = dict()\n",
    "    ret_hr['hashtag_dict'] = hashtag_dict\n",
    "    ret_hr['rdlist'] = rdlist\n",
    "    return ret_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(input_string, prefix):\n",
    "    if input_string.startswith(prefix):\n",
    "        line_new = input_string[len(prefix):]\n",
    "        return line_new\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetimeret(inp_date):\n",
    "    str_date = inp_date[60:70]  # 2017-08-24 18:56:46\n",
    "    # date_time_obj = datetime.strptime(date_time_str, '%d/%m/%y %H:%M:%S')\n",
    "    ret_date = datetime.strptime(str_date, '%Y-%m-%d %H:%M:%S')\n",
    "    return ret_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_ecd_filename_dict():\n",
    "    df = pd.read_csv(embedding_name_path, dtype=object)\n",
    "    dict1 = pd.Series(df.Embedding.values, index=df.encode_Hashtag).to_dict()\n",
    "    ecd_filename_dict = {}\n",
    "    for x, y in dict1.items():\n",
    "        x = remove_prefix(x, 'encode_')\n",
    "        x = x+'_data.csv'\n",
    "        ecd_filename_dict[x] = y\n",
    "    return ecd_filename_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_ecd_filename_dictr():\n",
    "    df = pd.read_csv(embedding_name_pathr, dtype=object)\n",
    "    dict1 = pd.Series(df.Embeddings.values,\n",
    "                      index=df.encode_re_Hashtag).to_dict()\n",
    "    ecd_filename_dictr = {}\n",
    "    for x, y in dict1.items():\n",
    "        x = remove_prefix(x, 'encode_re_')\n",
    "        x = x+'_rt_data.csv'\n",
    "        ecd_filename_dictr[x] = y\n",
    "    return ecd_filename_dictr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nodes():\n",
    "    ecd_filename_dict = initialize_ecd_filename_dict()\n",
    "    ecd_filename_dictr = initialize_ecd_filename_dictr()\n",
    "    ret_hr = initialize_hashtagdict_rdlist()\n",
    "    hashtag_dict = ret_hr['hashtag_dict']\n",
    "    rdlist = ret_hr['rdlist']\n",
    "    print(len(rdlist))\n",
    "    print(rdlist)\n",
    "    mapping_users = {}\n",
    "    user_features = []\n",
    "    tweet_hashtag = []\n",
    "    train_mask = []\n",
    "    test_mask = []\n",
    "    mapping_tweets = {}\n",
    "    cou_tweets = 0\n",
    "    cou_users = 0\n",
    "    tweet_features = []\n",
    "    hashtag_features = []\n",
    "    mapping_hashtag = {}\n",
    "    cou_hashtags = 0\n",
    "    count_missing_embedding = 0\n",
    "    incorrect_embedding = 0\n",
    "    # all files in a csv and give index\n",
    "    # code to give all files of a folder an index\n",
    "    # code to map each file name to a code\n",
    "    # iterrate based on that order\n",
    "    # x = [] #filename - csv encode it      x.append(model.encode(filename))\n",
    "    # y = [] #y.append(map[filename] coding])\n",
    "    filelist = []\n",
    "    missing_embedding = []\n",
    "    t = \"\"\n",
    "    nonexistfiles = []\n",
    "    for path in paths:\n",
    "        for filename in rdlist:\n",
    "            if (path == paths[1]):\n",
    "                filename1 = filename+'_rt_data.csv'\n",
    "                continue\n",
    "            else:\n",
    "                filename1 = filename+'_data.csv'\n",
    "\n",
    "            if os.path.exists(path+filename1) == False:\n",
    "                nonexistfiles.append(filename1)\n",
    "                continue\n",
    "            if t != \"\":\n",
    "                if t == filename:\n",
    "                    t = \"\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            df = pd.read_csv(path+filename1, dtype=object, lineterminator='\\n')\n",
    "\n",
    "            if ('user_tweet_count\\r' in df.columns):\n",
    "                df.rename(\n",
    "                    columns={'user_tweet_count\\r': 'user_tweet_count'}, inplace=True)\n",
    "            if ('user_tweet_count' not in df.columns):\n",
    "                filelist.append(filename1)\n",
    "                continue\n",
    "\n",
    "            df = df.dropna(subset=['tweet_id',\n",
    "                                   'author_id',\n",
    "                                   'mentions',\n",
    "                                   'retweet_count',\n",
    "                                   'like_count',\n",
    "                                   'quote_count',\n",
    "                                   'referenced_tweets',\n",
    "                                   'user_followers_count',\n",
    "                                   'user_following_count',\n",
    "                                   'user_listed_count',\n",
    "                                   'user_tweet_count'])\n",
    "            if (len(df) == 0):\n",
    "                filelist.append(filename1)\n",
    "                continue\n",
    "            embeddings = []\n",
    "            if filename1 not in ecd_filename_dict.keys() and filename1 not in ecd_filename_dictr.keys():\n",
    "                missing_embedding.append(filename1)\n",
    "                continue\n",
    "            elif filename1 in ecd_filename_dict.keys():\n",
    "                embed_filename = ecd_filename_dict[filename1]\n",
    "            elif filename1 in ecd_filename_dictr.keys():\n",
    "                embed_filename = ecd_filename_dictr[filename1]\n",
    "\n",
    "            try:\n",
    "                if filename1 in ecd_filename_dict.keys():\n",
    "                    embed_filename = embed_path+embed_filename\n",
    "                    embeddings = pd.read_csv(\n",
    "                        embed_filename, dtype=object, lineterminator='\\n')\n",
    "                elif filename1 in ecd_filename_dictr.keys():\n",
    "                    embed_filename = rembed_path+embed_filename\n",
    "                    embeddings = pd.read_csv(\n",
    "                        embed_filename, dtype=object, lineterminator='\\n')\n",
    "            except Exception as error:\n",
    "                print(\"ERROR:\", error)\n",
    "                continue\n",
    "\n",
    "            dict_embed = {}\n",
    "            for i in range(len(embeddings)):\n",
    "                dict_embed[embeddings['tweet_id'][i]] = re.findall(\n",
    "                    r\"[-]?\\d+.\\d+[e]?[-]?\\d+\", embeddings['Embeddings'][i])\n",
    "            h = []\n",
    "            tempfname = filename\n",
    "            findex = rdlist.index(filename)\n",
    "            if findex not in mapping_hashtag:\n",
    "                mapping_hashtag[findex] = cou_hashtags\n",
    "                hashtag_features.append(hashtag_dict[rdlist[findex]])\n",
    "                rand_test = random.randint(0, 100000)\n",
    "                train_mask.append(rand_test%2)\n",
    "                test_mask.append((rand_test+1)%2)\n",
    "                cou_hashtags = cou_hashtags + 1\n",
    "\n",
    "            a = []\n",
    "            df['referenced_tweets'] = df['referenced_tweets'].replace(\n",
    "                {'\\'': '\"'}, regex=True)\n",
    "            df['mentions'] = df['mentions'].replace({'\\'': '\"'}, regex=True)\n",
    "            f = 0\n",
    "            #\n",
    "            for i in range(0, len(df)):\n",
    "                try:\n",
    "                    if (type(df[\"mentions\"][i]) != str):\n",
    "                        df[\"mentions\"][i] = []\n",
    "                    else:\n",
    "                        df[\"mentions\"][i] = json.loads(df[\"mentions\"][i])\n",
    "                    l = []\n",
    "                    if df[\"author_id\"][i] not in mapping_users:\n",
    "                        mapping_users[int(\n",
    "                            float(df[\"author_id\"][i]))] = cou_users\n",
    "                        l.append(int(float(df[\"user_followers_count\"][i])))\n",
    "                        l.append(int(float(df[\"user_following_count\"][i])))\n",
    "                        # l.append(app_date)\n",
    "                        l.append(int(float(df[\"user_listed_count\"][i])))\n",
    "                        l.append(int(float(df[\"user_tweet_count\"][i])))\n",
    "                        user_features.append(l)\n",
    "                        cou_users = cou_users+1\n",
    "\n",
    "                    if (type(df[\"referenced_tweets\"][i]) != str):\n",
    "                        df[\"referenced_tweets\"][i] = []\n",
    "                    else:\n",
    "                        df[\"referenced_tweets\"][i] = json.loads(\n",
    "                            df[\"referenced_tweets\"][i])\n",
    "\n",
    "                    ttype = 1\n",
    "\n",
    "                    if (len(df[\"referenced_tweets\"][i]) > 0):\n",
    "                        if (df[\"referenced_tweets\"][i][0][\"type\"] == \"quoted\"):\n",
    "                            a.append('q')\n",
    "                            ttype = 1\n",
    "                            # print('q')\n",
    "                        elif (df[\"referenced_tweets\"][i][0][\"type\"] == \"replied_to\"):\n",
    "                            a.append('c')\n",
    "                            ttype = 2\n",
    "                            # print('c')\n",
    "                        elif (df[\"referenced_tweets\"][i][0][\"type\"] == \"retweeted\"):\n",
    "                            a.append('r')\n",
    "                            ttype = 3\n",
    "                            # print('r')\n",
    "                    else:\n",
    "                        a.append('o')\n",
    "                        ttype = 4\n",
    "                        # print('o')\n",
    "                    if ((int(float(df[\"tweet_id\"][i])) not in mapping_tweets) and (a[i] == 'q' or a[i] == 'o' or a[i] == 'c' or a[i] == 'r')):\n",
    "                        dic = np.zeros(768)\n",
    "                        if (df[\"tweet_id\"][i] in dict_embed):\n",
    "                            dic = np.array(dict_embed[df[\"tweet_id\"][i]])\n",
    "                        else:\n",
    "                            count_missing_embedding += 1\n",
    "                        dic1 = dic.astype(np.float)\n",
    "                        if (dic1.size < 768):\n",
    "                            incorrect_embedding += 1\n",
    "                        while (dic1.size < 768):\n",
    "                            dic1 = np.append(dic1, [0])\n",
    "                        dic1 = np.append(dic1, [ttype, float(df[\"retweet_count\"][i]), float(\n",
    "                            df[\"like_count\"][i]), float(df[\"quote_count\"][i])])\n",
    "                        v = dic1.tolist()\n",
    "                        tweet_features.append(v)\n",
    "                        tweet_hashtag.append(hashtag_dict[rdlist[findex]])\n",
    "                        mapping_tweets[int(\n",
    "                            float(df[\"tweet_id\"][i]))] = cou_tweets\n",
    "\n",
    "                        cou_tweets = cou_tweets+1\n",
    "                except Exception as error:\n",
    "                    print(\"ERROR:\", error)\n",
    "                    f = f+1\n",
    "\n",
    "    ret_nodes = dict()\n",
    "    ret_nodes['tweet_hashtag'] = tweet_hashtag\n",
    "    ret_nodes['mapping_users'] = mapping_users\n",
    "    ret_nodes['test_mask'] = test_mask\n",
    "    ret_nodes['train_mask'] = train_mask\n",
    "    ret_nodes['user_features'] = user_features\n",
    "    ret_nodes['cou_users'] = cou_users\n",
    "    ret_nodes['mapping_tweets'] = mapping_tweets\n",
    "    ret_nodes['tweet_features'] = tweet_features\n",
    "\n",
    "    ret_nodes['cou_tweets'] = cou_tweets\n",
    "    ret_nodes['mapping_hashtag'] = mapping_hashtag\n",
    "    ret_nodes['hashtag_features'] = hashtag_features\n",
    "    ret_nodes['cou_hashtags'] = cou_hashtags\n",
    "\n",
    "    pickle.dump(ret_nodes, open(\"ret_nodes.pickle\", \"wb\"), protocol=4)\n",
    "    print(\"incorrect embeddings: \", incorrect_embedding)\n",
    "    print(\"missing embeddings: \", count_missing_embedding)\n",
    "    return ret_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da353ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edges():\n",
    "    nonexistfiles = []\n",
    "    missing_embedding = []\n",
    "    ecd_filename_dict = initialize_ecd_filename_dict()\n",
    "    ecd_filename_dictr = initialize_ecd_filename_dictr()\n",
    "    ret_hr = initialize_hashtagdict_rdlist()\n",
    "    hashtag_dict = ret_hr['hashtag_dict']\n",
    "    rdlist = ret_hr['rdlist']\n",
    "\n",
    "    t = \"\"\n",
    "\n",
    "    with open('ret_nodes.pickle', 'rb') as handle:\n",
    "        ret_nodes = pickle.load(handle)\n",
    "\n",
    "    mapping_tweets = ret_nodes['mapping_tweets']\n",
    "    mapping_users = ret_nodes['mapping_users']\n",
    "    mapping_hashtag = ret_nodes['mapping_hashtag']\n",
    "\n",
    "    edge_user_qtweet_src = []\n",
    "    edge_user_qtweet_dst = []\n",
    "    label_user_qtweet = []\n",
    "\n",
    "    edge_user_otweet_src = []\n",
    "    edge_user_otweet_dst = []\n",
    "    label_user_otweet = []\n",
    "\n",
    "    edge_user_ctweet_src = []\n",
    "    edge_user_ctweet_dst = []\n",
    "    label_user_ctweet = []\n",
    "\n",
    "    edge_user_rtweet_src = []\n",
    "    edge_user_rtweet_dst = []\n",
    "    label_user_rtweet = []\n",
    "\n",
    "    edge_rtweet_tweet_src = []\n",
    "    edge_rtweet_tweet_dst = []\n",
    "    label_rtweet_tweet = []\n",
    "\n",
    "    edge_mention_src = []  # tweet to user\n",
    "    edge_mention_dst = []\n",
    "    label_mention = []\n",
    "\n",
    "    edge_qtweet_tweet_src = []\n",
    "    edge_qtweet_tweet_dst = []\n",
    "    label_qtweet_tweet = []\n",
    "\n",
    "    edge_ctweet_tweet_src = []\n",
    "    edge_ctweet_tweet_dst = []\n",
    "    label_ctweet_tweet = []\n",
    "\n",
    "    to_scrape_tweets = []\n",
    "    to_scrape_users = []\n",
    "\n",
    "    edge_hashtag_tweet_src = []\n",
    "    edge_hashtag_tweet_dst = []\n",
    "    label_hashtag_tweet = []\n",
    "\n",
    "    for path in paths:\n",
    "        for filename in rdlist:\n",
    "            if (path == paths[1]):\n",
    "                filename1 = filename+'_rt_data.csv'\n",
    "                continue\n",
    "            else:\n",
    "                filename1 = filename+'_data.csv'\n",
    "\n",
    "            if os.path.exists(path+filename1) == False:\n",
    "                nonexistfiles.append(filename1)\n",
    "                continue\n",
    "            if t != \"\":\n",
    "                if t == filename:\n",
    "                    t = \"\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            df = pd.read_csv(path+filename1, dtype=object, lineterminator='\\n')\n",
    "            if ('user_tweet_count\\r' in df.columns):\n",
    "                df.rename(\n",
    "                    columns={'user_tweet_count\\r': 'user_tweet_count'}, inplace=True)\n",
    "            if ('user_tweet_count' not in df.columns):\n",
    "                continue\n",
    "            if filename1 not in ecd_filename_dict.keys() and filename1 not in ecd_filename_dictr.keys():\n",
    "                missing_embedding.append(filename1)\n",
    "                continue\n",
    "            df = df.dropna(subset=['tweet_id',\n",
    "                                   'author_id',\n",
    "                                   'mentions',\n",
    "                                   'retweet_count',\n",
    "                                   'like_count',\n",
    "                                   'quote_count',\n",
    "                                   'referenced_tweets',\n",
    "                                   'user_followers_count',\n",
    "                                   'user_following_count',\n",
    "                                   'user_listed_count',\n",
    "                                   'user_tweet_count'])\n",
    "            df['mentions'] = df['mentions'].replace({'\\'': '\"'}, regex=True)\n",
    "\n",
    "            for i in range(0, len(df)):\n",
    "                if (type(df[\"mentions\"][i]) != str):\n",
    "                    df[\"mentions\"][i] = []\n",
    "                else:\n",
    "                    df[\"mentions\"][i] = json.loads(df[\"mentions\"][i])\n",
    "\n",
    "            cou = 0\n",
    "\n",
    "            df['referenced_tweets'] = df['referenced_tweets'].replace(\n",
    "                {'\\'': '\"'}, regex=True)\n",
    "\n",
    "            # for i in range(0,len(df)):\n",
    "            #    df[\"referenced_tweets\"][i]=json.loads(df[\"referenced_tweets\"][i])\n",
    "\n",
    "            for i in range(0, len(df)):\n",
    "                if (type(df[\"referenced_tweets\"][i]) != str):\n",
    "                    df[\"referenced_tweets\"][i] = []\n",
    "                else:\n",
    "                    df[\"referenced_tweets\"][i] = json.loads(\n",
    "                        df[\"referenced_tweets\"][i])\n",
    "\n",
    "            a = []\n",
    "            for i in range(0, len(df)):\n",
    "                if (len(df[\"referenced_tweets\"][i]) > 0):\n",
    "                    if (df[\"referenced_tweets\"][i][0][\"type\"] == \"quoted\"):\n",
    "                        a.append('q')\n",
    "                    elif (df[\"referenced_tweets\"][i][0][\"type\"] == \"replied_to\"):\n",
    "                        a.append('c')\n",
    "                    elif (df[\"referenced_tweets\"][i][0][\"type\"] == \"retweeted\"):\n",
    "                        a.append('r')\n",
    "                else:\n",
    "                    a.append('o')\n",
    "            df[\"tweet_type\"] = a\n",
    "\n",
    "            tempfname = filename\n",
    "            findex = rdlist.index(filename)\n",
    "\n",
    "            for i in range(0, len(df)):\n",
    "                if (df[\"tweet_type\"][i] == 'r'):\n",
    "                    label_user_rtweet.append(1)\n",
    "                    edge_user_rtweet_src.append(\n",
    "                        mapping_users[float(df[\"author_id\"][i])])\n",
    "                    edge_user_rtweet_dst.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "\n",
    "                    label_hashtag_tweet.append(1)\n",
    "                    edge_hashtag_tweet_dst.append(mapping_hashtag[findex])\n",
    "                    edge_hashtag_tweet_src.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "\n",
    "                    if df[\"referenced_tweets\"][i][0]['id'] in mapping_tweets:\n",
    "                        label_rtweet_tweet.append(1)\n",
    "                        edge_rtweet_tweet_src.append(\n",
    "                            mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "                        edge_rtweet_tweet_dst.append(\n",
    "                            mapping_tweets[int(df[\"referenced_tweets\"][i][0]['id'])])\n",
    "                    else:\n",
    "                        to_scrape_tweets.append(\n",
    "                            df[\"referenced_tweets\"][i][0]['id'])\n",
    "\n",
    "                elif df[\"tweet_type\"][i] == 'q':\n",
    "                    label_user_qtweet.append(1)\n",
    "                    edge_user_qtweet_src.append(\n",
    "                        mapping_users[float(df[\"author_id\"][i])])\n",
    "                    edge_user_qtweet_dst.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "\n",
    "                    label_hashtag_tweet.append(1)\n",
    "                    edge_hashtag_tweet_dst.append(mapping_hashtag[findex])\n",
    "                    edge_hashtag_tweet_src.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "                    if int(df[\"referenced_tweets\"][i][0]['id']) in mapping_tweets:\n",
    "                        label_qtweet_tweet.append(1)\n",
    "                        edge_qtweet_tweet_src.append(\n",
    "                            mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "                        edge_qtweet_tweet_dst.append(\n",
    "                            mapping_tweets[int(df[\"referenced_tweets\"][i][0]['id'])])\n",
    "                    else:\n",
    "                        to_scrape_tweets.append(\n",
    "                            df[\"referenced_tweets\"][i][0]['id'])\n",
    "\n",
    "                    for j in range(1, len(df[\"mentions\"][i])):\n",
    "                        if float(df[\"mentions\"][i][j][\"id\"]) in mapping_users:\n",
    "                            label_mention.append(1)\n",
    "                            edge_mention_src.append(\n",
    "                                mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "                            edge_mention_dst.append(\n",
    "                                mapping_users[float(df[\"mentions\"][i][j][\"id\"])])\n",
    "                        else:\n",
    "                            to_scrape_users.append(df[\"mentions\"][i][j][\"id\"])\n",
    "\n",
    "                elif df[\"tweet_type\"][i] == 'o':\n",
    "                    label_user_otweet.append(1)\n",
    "                    edge_user_otweet_src.append(\n",
    "                        mapping_users[float(df[\"author_id\"][i])])\n",
    "                    edge_user_otweet_dst.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "\n",
    "                    label_hashtag_tweet.append(1)\n",
    "                    edge_hashtag_tweet_dst.append(mapping_hashtag[findex])\n",
    "                    edge_hashtag_tweet_src.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "\n",
    "                    for j in range(1, len(df[\"mentions\"][i])):\n",
    "                        if float(df[\"mentions\"][i][j][\"id\"]) in mapping_users:\n",
    "                            label_mention.append(1)\n",
    "                            edge_mention_src.append(\n",
    "                                mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "                            edge_mention_dst.append(\n",
    "                                mapping_users[float(df[\"mentions\"][i][j][\"id\"])])\n",
    "                        else:\n",
    "                            to_scrape_users.append(df[\"mentions\"][i][j][\"id\"])\n",
    "\n",
    "                elif df[\"tweet_type\"][i] == 'c':\n",
    "                    label_user_ctweet.append(1)\n",
    "                    edge_user_ctweet_src.append(\n",
    "                        mapping_users[float(df[\"author_id\"][i])])\n",
    "                    edge_user_ctweet_dst.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "\n",
    "                    label_hashtag_tweet.append(1)\n",
    "                    edge_hashtag_tweet_dst.append(mapping_hashtag[findex])\n",
    "                    edge_hashtag_tweet_src.append(\n",
    "                        mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "\n",
    "                    if float(df[\"referenced_tweets\"][i][0]['id']) in mapping_tweets:\n",
    "                        label_ctweet_tweet.append(1)\n",
    "                        # CHANGED FROM edge_hashtag_tweet_src\n",
    "                        edge_ctweet_tweet_src.append(mapping_hashtag[findex])\n",
    "                        edge_ctweet_tweet_dst.append(\n",
    "                            mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "                    else:\n",
    "                        to_scrape_tweets.append(\n",
    "                            df[\"referenced_tweets\"][i][0]['id'])\n",
    "\n",
    "                    for j in range(1, len(df[\"mentions\"][i])):\n",
    "                        if float(df[\"mentions\"][i][j][\"id\"]) in mapping_users:\n",
    "                            label_mention.append(1)\n",
    "                            edge_mention_src.append(\n",
    "                                mapping_tweets[float(df[\"tweet_id\"][i])])\n",
    "                            edge_mention_dst.append(\n",
    "                                mapping_users[float(df[\"mentions\"][i][j][\"id\"])])\n",
    "                        else:\n",
    "                            to_scrape_users.append(df[\"mentions\"][i][j][\"id\"])\n",
    "                else:\n",
    "                    print(\"Nothing happened: \"+str(i))\n",
    "\n",
    "    ret_edges = dict()\n",
    "\n",
    "    ret_edges['edge_user_qtweet_src'] = edge_user_qtweet_src\n",
    "    ret_edges['edge_user_qtweet_dst'] = edge_user_qtweet_dst\n",
    "    ret_edges['label_user_qtweet'] = label_user_qtweet\n",
    "\n",
    "    ret_edges['edge_user_otweet_src'] = edge_user_otweet_src\n",
    "    ret_edges['edge_user_otweet_dst'] = edge_user_otweet_dst\n",
    "    ret_edges['label_user_otweet'] = label_user_otweet\n",
    "\n",
    "    ret_edges['edge_user_ctweet_src'] = edge_user_ctweet_src\n",
    "    ret_edges['edge_user_ctweet_dst'] = edge_user_ctweet_dst\n",
    "    ret_edges['label_user_ctweet'] = label_user_ctweet\n",
    "\n",
    "    ret_edges['edge_user_rtweet_src'] = edge_user_rtweet_src\n",
    "    ret_edges['edge_user_rtweet_dst'] = edge_user_rtweet_dst\n",
    "    ret_edges['label_user_rtweet'] = label_user_rtweet\n",
    "\n",
    "    ret_edges['edge_qtweet_tweet_src'] = edge_qtweet_tweet_src\n",
    "    ret_edges['edge_qtweet_tweet_dst'] = edge_qtweet_tweet_dst\n",
    "    ret_edges['label_qtweet_tweet'] = label_qtweet_tweet\n",
    "\n",
    "    ret_edges['edge_ctweet_tweet_src'] = edge_ctweet_tweet_src\n",
    "    ret_edges['edge_ctweet_tweet_dst'] = edge_ctweet_tweet_dst\n",
    "    ret_edges['label_ctweet_tweet'] = label_ctweet_tweet\n",
    "\n",
    "    ret_edges['edge_mention_src'] = edge_mention_src\n",
    "    ret_edges['edge_mention_dst'] = edge_mention_dst\n",
    "    ret_edges['label_mention'] = label_mention\n",
    "\n",
    "    ret_edges['edge_hashtag_tweet_src'] = edge_hashtag_tweet_src\n",
    "    ret_edges['edge_hashtag_tweet_dst'] = edge_hashtag_tweet_dst\n",
    "    ret_edges['label_hashtag_tweet'] = label_hashtag_tweet\n",
    "\n",
    "    ret_edges['to_scrape_tweets'] = to_scrape_tweets\n",
    "    ret_edges['to_scrape_users'] = to_scrape_users\n",
    "\n",
    "    pickle.dump(ret_edges, open(\"ret_edges.pickle\", \"wb\"), protocol=4)\n",
    "\n",
    "    return ret_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEdgeAndLabel(edge_from_to_src, edge_from_to_dst, label_from_to):\n",
    "    edge_from_to_src = np.array(edge_from_to_src)\n",
    "    edge_from_to_src = edge_from_to_src.reshape((edge_from_to_src.shape[0], 1))\n",
    "\n",
    "    edge_from_to_dst = np.array(edge_from_to_dst)\n",
    "    edge_from_to_dst = edge_from_to_dst.reshape((edge_from_to_dst.shape[0], 1))\n",
    "\n",
    "    edge_from_to = [edge_from_to_src, edge_from_to_dst]\n",
    "    edge_from_to = np.array(edge_from_to)\n",
    "    edge_from_to = edge_from_to.reshape(\n",
    "        (edge_from_to.shape[0], edge_from_to.shape[1]))\n",
    "    edge_from_to = torch.tensor(edge_from_to.astype('int64'))\n",
    "\n",
    "    label_from_to = np.array(label_from_to)\n",
    "    label_from_to = torch.tensor(label_from_to.reshape(\n",
    "        (label_from_to.shape[0], 1)).astype('int64'))\n",
    "    return edge_from_to, label_from_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    with open('ret_edges.pickle', 'rb') as handle:\n",
    "        ret_edges = pickle.load(handle)\n",
    "\n",
    "    edge_user_otweet, label_user_otweet = getEdgeAndLabel(\n",
    "        ret_edges['edge_user_otweet_src'], ret_edges['edge_user_otweet_dst'], ret_edges['label_user_otweet'])\n",
    "    edge_user_ctweet, label_user_ctweet = getEdgeAndLabel(\n",
    "        ret_edges['edge_user_ctweet_src'], ret_edges['edge_user_ctweet_dst'], ret_edges['label_user_ctweet'])\n",
    "    edge_user_qtweet, label_user_qtweet = getEdgeAndLabel(\n",
    "        ret_edges['edge_user_qtweet_src'], ret_edges['edge_user_qtweet_dst'], ret_edges['label_user_qtweet'])\n",
    "    edge_user_rtweet, label_user_rtweet = getEdgeAndLabel(\n",
    "        ret_edges['edge_user_rtweet_src'], ret_edges['edge_user_rtweet_dst'], ret_edges['label_user_rtweet'])\n",
    "    edge_ctweet_tweet, label_ctweet_tweet = getEdgeAndLabel(\n",
    "        ret_edges['edge_ctweet_tweet_src'], ret_edges['edge_ctweet_tweet_dst'], ret_edges['label_ctweet_tweet'])\n",
    "    edge_qtweet_tweet, label_qtweet_tweet = getEdgeAndLabel(\n",
    "        ret_edges['edge_qtweet_tweet_src'], ret_edges['edge_qtweet_tweet_dst'], ret_edges['label_qtweet_tweet'])\n",
    "    edge_mention, label_mention = getEdgeAndLabel(\n",
    "        ret_edges['edge_mention_src'], ret_edges['edge_mention_dst'], ret_edges['label_mention'])\n",
    "    edge_hashtag_tweet, label_hashtag_tweet = getEdgeAndLabel(\n",
    "        ret_edges['edge_hashtag_tweet_src'], ret_edges['edge_hashtag_tweet_dst'], ret_edges['label_hashtag_tweet'])\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    with open('ret_nodes.pickle', 'rb') as handle1:\n",
    "        ret_nodes = pickle.load(handle1)\n",
    "    user_features = ret_nodes['user_features']\n",
    "    tweet_features = ret_nodes['tweet_features']\n",
    "    tweet_hashtag = ret_nodes['tweet_hashtag']\n",
    "    hashtag_features = ret_nodes['hashtag_features']\n",
    "    train_mask = ret_nodes['train_mask']\n",
    "    test_mask = ret_nodes['test_mask']\n",
    "    train_mask = torch.tensor(train_mask)\n",
    "    test_mask = torch.tensor(test_mask)\n",
    "#    tweet_types=ret_nodes['tweet_types']\n",
    "#    retweets_count=ret_nodes['retweets_count']\n",
    "#    tweet_like_count=ret_nodes['tweet_like_count']\n",
    "#    tweet_quote_count=ret_nodes['tweet_quote_count']\n",
    "    user_features = torch.tensor(user_features).float()\n",
    "    # print(tweet_types)\n",
    "#    tweet_types=torch.tensor(tweet_types)\n",
    "#    retweets_count=torch.tensor(retweets_count)\n",
    "#    tweet_like_count=torch.tensor(tweet_like_count)\n",
    "    tweet_hashtag = torch.tensor(tweet_hashtag).float()\n",
    "    tweet_features = torch.tensor(tweet_features).float()\n",
    "#    tweet_quote_count=torch.tensor(tweet_quote_count)\n",
    "    hashtag_features = torch.tensor(hashtag_features).float()\n",
    "\n",
    "    data['user'].x = user_features\n",
    "    data['tweet'].x = tweet_features\n",
    "    #data['tweet'].y = tweet_hashtag\n",
    "    # data['tweet'].tweet_type=tweet_types\n",
    "    # data['tweet'].retweet_count=retweets_count\n",
    "    # data['tweet'].like_count=tweet_like_count\n",
    "    # data['tweet'].quote_count=tweet_quote_count\n",
    "\n",
    "    data['hashtag'].y=hashtag_features\n",
    "    data['hashtag'].train_mask = train_mask\n",
    "    data['hashtag'].test_mask = test_mask\n",
    "\n",
    "    data['user', 'post_original', 'tweet'].edge_index = edge_user_otweet\n",
    "    # data['user','post_original','tweet'].edge_label=label_user_otweet\n",
    "\n",
    "    data['user', 'post_quote', 'tweet'].edge_index = edge_user_qtweet\n",
    "    # data['user','post_quote','tweet'].edge_label=label_user_qtweet\n",
    "\n",
    "    data['user', 'post_reply', 'tweet'].edge_index = edge_user_ctweet\n",
    "    # data['user','post_reply','tweet'].edge_label=label_user_ctweet\n",
    "\n",
    "    # data['user','post_retweet','tweet'].edge_index=edge_user_rtweet\n",
    "    # data['user','post_retweet','tweet'].edge_label=label_user_rtweet\n",
    "\n",
    "    data['tweet', 'quotes', 'tweet'].edge_index = edge_qtweet_tweet\n",
    "    # data['tweet','quotes','tweet'].edge_label=label_qtweet_tweet\n",
    "\n",
    "    data['tweet', 'replies', 'tweet'].edge_index = edge_ctweet_tweet\n",
    "    # data['tweet','replies','tweet'].edge_label=label_ctweet_tweet\n",
    "\n",
    "    data['tweet', 'mentions', 'user'].edge_index = edge_mention\n",
    "    # data['tweet','mentions','user'].edge_label=label_mention\n",
    "\n",
    "    data['tweet','hashtagedge','hashtag'].edge_index=edge_hashtag_tweet\n",
    "    # data['hashtag','hashtagedge','tweet'].edge_label=label_hashtag_tweet\n",
    "    print(data)\n",
    "    pickle.dump(data, open(\"GRAPH_HASHTAG_LARGE.pickle\", \"wb\"), protocol=4)\n",
    "    return data\n",
    "\n",
    "# NODES = build_nodes()\n",
    "# print(\"NODES FORMATION COMPLETE\")\n",
    "# EDGES = build_edges()\n",
    "# print(\"EDGE FORMATION COMPLETE\")\n",
    "# GRAPH = build_graph()\n",
    "# print(\"GRAPH FORMATION COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5426e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GNN_Metadata_PreProcessing():\n",
    "    node_types = ['user', 'tweet']\n",
    "    edge_types = [\n",
    "        ('user', 'post_original', 'tweet'),\n",
    "        ('user', 'post_quote', 'tweet'),\n",
    "        ('user', 'post_reply', 'tweet'),\n",
    "        ('tweet', 'quotes', 'tweet'),\n",
    "        ('tweet', 'replies', 'tweet'),\n",
    "        ('tweet', 'mentions', 'user')]\n",
    "    metadata = (node_types, edge_types)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConvWithMultipleLinearLayers(torch.nn.Module):\n",
    "    def __init__(self, num_classes=5, hidden_dim=256, num_hidden_layers=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), 512)\n",
    "        self.conv2 = SAGEConv((-1, -1), 256)\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x).relu()\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return torch.nn.functional.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ce5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATWithMultipleLinearLayers(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=512, out_channels=256, hidden_dim=256, num_hidden_layers=3, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.lin1 = Linear(-1, hidden_channels)\n",
    "        self.conv2 = GATConv((-1, -1), out_channels, add_self_loops=False)\n",
    "        self.lin2 = Linear(-1, out_channels)\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x).relu()\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return torch.nn.functional.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241755dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=512, out_channels=5, hidden_dim=256, num_hidden_layers=3, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.lin1 = Linear(-1, hidden_channels)\n",
    "        self.conv2 = GATConv((-1, -1), hidden_dim, add_self_loops=False)\n",
    "        self.lin2 = Linear(-1, hidden_dim)\n",
    "        self.conv3 = GATConv((-1, -1), out_channels, add_self_loops=False)\n",
    "        self.lin3 = Linear(-1, out_channels)\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index) + self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index) + self.lin2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b241849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSAGEConv(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=512, out_channels=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConvAndGATWithLinearLayers(torch.nn.Module):\n",
    "    def __init__(self, num_classes=5, hidden_dim=256, num_hidden_layers=4):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), 512)\n",
    "        self.conv2 = SAGEConv((-1, -1), 256)\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        for _ in range(3):\n",
    "            self.gat_layers.append(\n",
    "                GATConv((-1, -1), hidden_dim, add_self_loops=False))\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        for layer in self.gat_layers:\n",
    "            x = layer(x, edge_index).relu()\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x).relu()\n",
    "        x = self.fc(x)\n",
    "        return torch.nn.functional.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Preprocessing():\n",
    "#    model = modeType()\n",
    "#    metadata = GNN_Metadata_PreProcessing()\n",
    "#    model = to_hetero(model, metadata)\n",
    "    NODES = build_nodes()\n",
    "    print(\"NODES FORMATION COMPLETE\")\n",
    "    EDGES = build_edges()\n",
    "    print(\"EDGE FORMATION COMPLETE\")\n",
    "    build_graph()\n",
    "    print(\"GRAPH FORMATION COMPLETE\")\n",
    "#    GRAPH={}\n",
    "#    with open(\"GRAPH.pickle\",'rb') as f:\n",
    "#        GRAPH = pickle.load(f)\n",
    "#    x_dict = GRAPH.x_dict\n",
    "#    edge_index_dict = GRAPH.edge_index_dict\n",
    "   # model = modeType()\n",
    "#    metadata = GNN_Metadata_PreProcessing()\n",
    "#    model = to_hetero(model, metadata)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, targets):\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    correct_predictions = (predicted_classes == targets).sum().item()\n",
    "    total_predictions = targets.size(0)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model_architectures = [SimpleSAGEConv, SimpleGAT, SAGEConvWithMultipleLinearLayers,\n",
    "                           GATWithMultipleLinearLayers,  SAGEConvAndGATWithLinearLayers]\n",
    "    for a in model_architectures:\n",
    "        print(a)\n",
    "        GRAPH, x_dict, edge_index_dict, model = Model_Preprocessing(a)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        data = GRAPH.y_dict['tweet']\n",
    "        num_categories = 5\n",
    "        for epoch in range(10):\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(x_dict, edge_index_dict)\n",
    "            out = out['tweet']\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(out, data.long())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            accuracy = compute_accuracy(out, data)\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "\n",
    "            if (epoch == 9):\n",
    "                with open(\"results.txt\", \"a\") as file:\n",
    "                    file.write(\n",
    "                        f\"model: {a.__name__}, Loss: {loss.item()}, Accuracy: {accuracy}\\n\")\n",
    "\n",
    "\n",
    "Model_Preprocessing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
